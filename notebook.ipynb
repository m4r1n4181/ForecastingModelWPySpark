{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6918e18a-c248-4929-b552-7aee2057c0eb",
   "metadata": {},
   "source": [
    "## The Data\n",
    "\n",
    "# Online Retail.csv\n",
    "\n",
    "| Column     | Description              |\n",
    "|------------|--------------------------|\n",
    "| `'InvoiceNo'` | A 6-digit number uniquely assigned to each transaction |\n",
    "| `'StockCode'` | A 5-digit number uniquely assigned to each distinct product |\n",
    "| `'Description'` | The product name |\n",
    "| `'Quantity'` | The quantity of each product (item) per transaction |\n",
    "| `'UnitPrice'` | Product price per unit |\n",
    "| `'CustomerID'` | A 5-digit number uniquely assigned to each customer |\n",
    "| `'Country'` | The name of the country where each customer resides |\n",
    "| `'InvoiceDate'` | The day and time when each transaction was generated `\"MM/DD/YYYY\"` |\n",
    "| `'Year'` | The year when each transaction was generated |\n",
    "| `'Month'` | The month when each transaction was generated |\n",
    "| `'Week'` | The week when each transaction was generated (`1`-`52`) |\n",
    "| `'Day'` | The day of the month when each transaction was generated (`1`-`31`) |\n",
    "| `'DayOfWeek'` | The day of the weeke when each transaction was generated <br>(`0` = Monday, `6` = Sunday) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62329aaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in c:\\users\\makin\\anaconda3\\lib\\site-packages (3.5.4)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in c:\\users\\makin\\anaconda3\\lib\\site-packages (from pyspark) (0.10.9.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81a07c66-a3d4-4fdd-9c3c-7b3a19b80d62",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 855,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "lastExecutedAt": 1739387294315,
    "lastExecutedByKernel": "95fcbd91-5b12-47e5-8aad-c56f8cfa4f1d",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Import required libraries\nfrom pyspark.sql import SparkSession\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.regression import RandomForestRegressor\nfrom pyspark.sql.functions import col, dayofmonth, month, year,  to_date, to_timestamp, weekofyear, dayofweek\nfrom pyspark.ml.feature import StringIndexer\nfrom pyspark.ml.evaluation import RegressionEvaluator\n\n# Initialize Spark session\nmy_spark = SparkSession.builder.appName(\"SalesForecast\").getOrCreate()\n\n# Importing sales data\nsales_data = my_spark.read.csv(\n    \"Online Retail.csv\", header=True, inferSchema=True, sep=\",\")\n\n# Convert InvoiceDate to datetime \nsales_data = sales_data.withColumn(\"InvoiceDate\", to_date(\n    to_timestamp(col(\"InvoiceDate\"), \"d/M/yyyy H:mm\")))",
    "outputsMetadata": {
     "0": {
      "height": 38,
      "type": "stream"
     },
     "1": {
      "height": 59,
      "type": "stream"
     },
     "2": {
      "height": 38,
      "type": "stream"
     }
    }
   },
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.sql.functions import col, dayofmonth, month, year,  to_date, to_timestamp, weekofyear, dayofweek\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Initialize Spark session\n",
    "my_spark = SparkSession.builder.appName(\"SalesForecast\").getOrCreate()\n",
    "\n",
    "# Importing sales data\n",
    "sales_data = my_spark.read.csv(\n",
    "    \"Online_Retail.csv\", header=True, inferSchema=True, sep=\",\")\n",
    "\n",
    "# Convert InvoiceDate to datetime \n",
    "sales_data = sales_data.withColumn(\"InvoiceDate\", to_date(\n",
    "    to_timestamp(col(\"InvoiceDate\"), \"d/M/yyyy H:mm\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5106e04-f9da-459f-a1cc-14e437fe001d",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 12938,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "lastExecutedAt": 1739387307254,
    "lastExecutedByKernel": "95fcbd91-5b12-47e5-8aad-c56f8cfa4f1d",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "from pyspark.sql.functions import sum as sum, avg\nfrom pyspark.ml.feature import StringIndexer, VectorAssembler\nfrom pyspark.ml.regression import RandomForestRegressor\nfrom pyspark.ml import Pipeline\n\n# Correcting the column names in the groupBy and agg functions\n# Aggregate data into daily intervals\ndaily_sales_data = sales_data.groupBy(\"Country\", \"StockCode\", \"InvoiceDate\", \"Year\", \"Month\", \"Day\", \"Week\", \"DayOfWeek\").agg({\"Quantity\": \"sum\", \"UnitPrice\": \"avg\"})\n# Rename the target column\ndaily_sales_data = daily_sales_data.withColumnRenamed(\"sum(Quantity)\", \"Quantity\").withColumnRenamed(\"avg(UnitPrice)\", \"UnitPrice\")\n    \nsplitting_date = \"2011-09-25\"\n\ntrain_data = daily_sales_data.filter(daily_sales_data.InvoiceDate < splitting_date)\ntest_data = daily_sales_data.filter(daily_sales_data.InvoiceDate >= splitting_date)\n\n# Convert to Pandas DataFrame for local processing\npd_daily_train_data = train_data.toPandas()\n\n# Create a StringIndexer\ncountry_indexer = StringIndexer(inputCol=\"Country\", outputCol=\"country_index\").setHandleInvalid(\"keep\")\nstock_code_indexer = StringIndexer(inputCol=\"StockCode\", outputCol=\"stock_code_index\").setHandleInvalid(\"keep\")\n\n# Make a VectorAssembler\nassembler = VectorAssembler(inputCols=[\"Month\", \"Day\", \"DayOfWeek\", \"Week\", \"country_index\", \"stock_code_index\"], outputCol=\"features\")\nrf = RandomForestRegressor(featuresCol=\"features\",labelCol=\"Quantity\", maxBins = 4000)\n\nsales_pipe = Pipeline(stages=[country_indexer, stock_code_indexer, assembler, rf])\n\n# Fit the model using Spark DataFrame\nmodel = sales_pipe.fit(train_data)",
    "outputsMetadata": {
     "0": {
      "height": 38,
      "type": "stream"
     },
     "1": {
      "height": 59,
      "type": "stream"
     },
     "2": {
      "height": 38,
      "type": "stream"
     },
     "3": {
      "height": 38,
      "type": "stream"
     },
     "4": {
      "height": 38,
      "type": "stream"
     }
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum as sum, avg\n",
    "\n",
    "# Correcting the column names in the groupBy and agg functions\n",
    "# Aggregate data into daily intervals\n",
    "daily_sales_data = sales_data.groupBy(\"Country\", \"StockCode\", \"InvoiceDate\", \"Year\", \"Month\", \"Day\", \"Week\", \"DayOfWeek\").agg({\"Quantity\": \"sum\", \"UnitPrice\": \"avg\"})\n",
    "# Rename the target column\n",
    "daily_sales_data = daily_sales_data.withColumnRenamed(\"sum(Quantity)\", \"Quantity\").withColumnRenamed(\"avg(UnitPrice)\", \"UnitPrice\")\n",
    "    \n",
    "splitting_date = \"2011-09-25\"\n",
    "\n",
    "train_data = daily_sales_data.filter(daily_sales_data.InvoiceDate < splitting_date)\n",
    "test_data = daily_sales_data.filter(daily_sales_data.InvoiceDate >= splitting_date)\n",
    "\n",
    "# Convert to Pandas DataFrame for local processing\n",
    "pd_daily_train_data = train_data.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f40a4830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a StringIndexer\n",
    "country_indexer = StringIndexer(inputCol=\"Country\", outputCol=\"country_index\").setHandleInvalid(\"keep\")\n",
    "stock_code_indexer = StringIndexer(inputCol=\"StockCode\", outputCol=\"stock_code_index\").setHandleInvalid(\"keep\")\n",
    "\n",
    "# Make a VectorAssembler\n",
    "assembler = VectorAssembler(inputCols=[\"Month\", \"Day\", \"DayOfWeek\", \"Week\", \"country_index\", \"stock_code_index\"], outputCol=\"features\")\n",
    "rf = RandomForestRegressor(featuresCol=\"features\",labelCol=\"Quantity\", maxBins = 4000)\n",
    "\n",
    "sales_pipe = Pipeline(stages=[country_indexer, stock_code_indexer, assembler, rf])\n",
    "\n",
    "# Fit the model using Spark DataFrame\n",
    "model = sales_pipe.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "176e8b9e-e1c3-4df8-923f-bfb87e5f0f45",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 269,
    "lastExecutedAt": 1739387307523,
    "lastExecutedByKernel": "95fcbd91-5b12-47e5-8aad-c56f8cfa4f1d",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "from pyspark.sql.functions import col\n\n# Assuming sales_pipe is a trained PipelineModel\ntest_predictions = model.transform(test_data)\ntest_predictions = test_predictions.withColumn(\"prediction\", col(\"prediction\").cast(\"double\"))"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Assuming sales_pipe is a trained PipelineModel\n",
    "test_predictions = model.transform(test_data)\n",
    "test_predictions = test_predictions.withColumn(\"prediction\", col(\"prediction\").cast(\"double\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3d4790f-64f6-499e-bd32-f5cd7d3ebfca",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 1289,
    "lastExecutedAt": 1739387308813,
    "lastExecutedByKernel": "95fcbd91-5b12-47e5-8aad-c56f8cfa4f1d",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "mae = RegressionEvaluator(labelCol = \"Quantity\", predictionCol = \"prediction\", metricName = \"mae\")\nmae = mae.evaluate(test_predictions)",
    "outputsMetadata": {
     "0": {
      "height": 38,
      "type": "stream"
     }
    }
   },
   "outputs": [],
   "source": [
    "# Calculating MEAN ABSOLUTE ERROR\n",
    "mae = RegressionEvaluator(labelCol = \"Quantity\", predictionCol = \"prediction\", metricName = \"mae\")\n",
    "mae = mae.evaluate(test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "428f49c4-ed58-4655-90a1-2e29a2bb506f",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 1268,
    "lastExecutedAt": 1739387310081,
    "lastExecutedByKernel": "95fcbd91-5b12-47e5-8aad-c56f8cfa4f1d",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "weekly_test_predictions = test_predictions.groupBy(\"Year\", \"Week\").agg({\"prediction\": \"sum\"})\n\n# Finding the quantity sold on the 39 week. \npromotion_week = weekly_test_predictions.filter(col('Week')==39)\n\n# Storing prediction as quantity_sold_w30\nquantity_sold_w39 = int(promotion_week.select(\"sum(prediction)\").collect()[0][0])\nprint(quantity_sold_w39)\nmy_spark.stop()",
    "outputsMetadata": {
     "0": {
      "height": 38,
      "type": "stream"
     },
     "1": {
      "height": 38,
      "type": "stream"
     },
     "2": {
      "height": 38,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87357\n"
     ]
    }
   ],
   "source": [
    "weekly_test_predictions = test_predictions.groupBy(\"Year\", \"Week\").agg({\"prediction\": \"sum\"})\n",
    "\n",
    "# Finding the quantity sold on the 39 week. \n",
    "promotion_week = weekly_test_predictions.filter(col('Week')==39)\n",
    "\n",
    "# Storing prediction as quantity_sold_w30\n",
    "quantity_sold_w39 = int(promotion_week.select(\"sum(prediction)\").collect()[0][0])\n",
    "print(quantity_sold_w39)\n",
    "my_spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bb016b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Welcome to DataCamp Workspaces.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
